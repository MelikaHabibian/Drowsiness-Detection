{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b28202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Conv2D,MaxPooling2D,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesful Image Import Count = 500\n",
      "Succesful Image Import Count = 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_images_from_folder(folder, eyes = 0):\n",
    "    count = 0\n",
    "    error_count = 0\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        try:\n",
    "            img = cv2.imread(os.path.join(folder,filename))\n",
    "            img = cv2.resize(img, (80,80)) ## Resizing the images\n",
    "            ## for eyes if it is 0: open, 1: close\n",
    "            images.append([img, eyes])\n",
    "        except:\n",
    "            error_count += 1\n",
    "            print('ErrorCount = ' + str(error_count))\n",
    "            continue\n",
    "        \n",
    "        count += 1\n",
    "        if count % 500 == 0:\n",
    "            print('Succesful Image Import Count = ' + str(count))\n",
    "\n",
    "    return images\n",
    "\n",
    "folder=\"Test_Dataset/Open_Eyes/\"\n",
    "open_eyes = load_images_from_folder(folder, 0)\n",
    "\n",
    "folder=\"Test_Dataset/Closed_Eyes/\"\n",
    "closed_eyes = load_images_from_folder(folder, 1)\n",
    "eyes = closed_eyes + open_eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e05e9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] \n",
    "y = [] \n",
    "for features, label in eyes: \n",
    "     X.append(features)\n",
    "     y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ffc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1, 80, 80, 3)\n",
    "y = np.array(y)\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641053d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe754297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "2/2 [==============================] - 94s 64s/step - loss: 0.6922 - auc: 0.5211 - val_loss: 0.6787 - val_auc: 0.8400\n",
      "Epoch 2/24\n",
      "2/2 [==============================] - 31s 15s/step - loss: 0.6779 - auc: 0.5933 - val_loss: 0.6363 - val_auc: 0.8278\n",
      "Epoch 3/24\n",
      "2/2 [==============================] - 31s 15s/step - loss: 0.6551 - auc: 0.6446 - val_loss: 0.5860 - val_auc: 0.8541\n",
      "Epoch 4/24\n",
      "2/2 [==============================] - 33s 15s/step - loss: 0.6122 - auc: 0.7173 - val_loss: 0.5396 - val_auc: 0.8735\n",
      "Epoch 5/24\n",
      "2/2 [==============================] - 30s 15s/step - loss: 0.5826 - auc: 0.7418 - val_loss: 0.4835 - val_auc: 0.8684\n",
      "Epoch 6/24\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.5190 - auc: 0.8043 - val_loss: 0.4486 - val_auc: 0.8953\n",
      "Epoch 7/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.5067 - auc: 0.8099 - val_loss: 0.3961 - val_auc: 0.8977\n",
      "Epoch 8/24\n",
      "2/2 [==============================] - 32s 17s/step - loss: 0.4445 - auc: 0.8596 - val_loss: 0.3416 - val_auc: 0.9066\n",
      "Epoch 9/24\n",
      "2/2 [==============================] - 33s 15s/step - loss: 0.3942 - auc: 0.8733 - val_loss: 0.3271 - val_auc: 0.9103\n",
      "Epoch 10/24\n",
      "2/2 [==============================] - 31s 14s/step - loss: 0.3740 - auc: 0.8780 - val_loss: 0.3277 - val_auc: 0.9064\n",
      "Epoch 11/24\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.3447 - auc: 0.8922 - val_loss: 0.3036 - val_auc: 0.9234\n",
      "Epoch 12/24\n",
      "2/2 [==============================] - 34s 16s/step - loss: 0.3322 - auc: 0.8921 - val_loss: 0.2721 - val_auc: 0.9239\n",
      "Epoch 13/24\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.3022 - auc: 0.9018 - val_loss: 0.2464 - val_auc: 0.9295\n",
      "Epoch 14/24\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.2725 - auc: 0.9059 - val_loss: 0.2372 - val_auc: 0.9420\n",
      "Epoch 15/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.2661 - auc: 0.9222 - val_loss: 0.2613 - val_auc: 0.9406\n",
      "Epoch 16/24\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.2486 - auc: 0.9331 - val_loss: 0.2034 - val_auc: 0.9563\n",
      "Epoch 17/24\n",
      "2/2 [==============================] - 35s 18s/step - loss: 0.2108 - auc: 0.9508 - val_loss: 0.1890 - val_auc: 0.9618\n",
      "Epoch 18/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.2132 - auc: 0.9563 - val_loss: 0.3061 - val_auc: 0.9496\n",
      "Epoch 19/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.2552 - auc: 0.9322 - val_loss: 0.1829 - val_auc: 0.9790\n",
      "Epoch 20/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.1823 - auc: 0.9725 - val_loss: 0.1860 - val_auc: 0.9770\n",
      "Epoch 21/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.1713 - auc: 0.9704 - val_loss: 0.1941 - val_auc: 0.9781\n",
      "Epoch 22/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.1684 - auc: 0.9818 - val_loss: 0.1625 - val_auc: 0.9829\n",
      "Epoch 23/24\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.1519 - auc: 0.9828 - val_loss: 0.1484 - val_auc: 0.9889\n",
      "Epoch 24/24\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.1470 - auc: 0.9863 - val_loss: 0.1374 - val_auc: 0.9836\n",
      "16/16 [==============================] - 2s 125ms/step - loss: 0.1374 - auc: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13738496601581573, 0.9836084246635437]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Adding first three convolutional layers\n",
    "model.add(Conv2D(\n",
    "                filters = 32, # number of filters\n",
    "                kernel_size = (3,3), # height/width of filter\n",
    "                activation = 'relu', # activation function \n",
    "                input_shape = (80,80,3) # shape of input (image)\n",
    "                ))\n",
    "model.add(Conv2D(\n",
    "                filters = 32, # number of filters\n",
    "                kernel_size = (3,3), # height/width of filter\n",
    "                activation = 'relu' # activation function \n",
    "                ))\n",
    "model.add(Conv2D(\n",
    "                filters = 32, # number of filters\n",
    "                kernel_size = (3,3), # height/width of filter\n",
    "                activation = 'relu' # activation function \n",
    "                ))\n",
    "\n",
    "# Adding pooling after convolutional layers\n",
    "model.add(MaxPooling2D(pool_size = (2,2))) # Dimensions of the region that you are pooling\n",
    "\n",
    "# Adding second set of convolutional layers\n",
    "model.add(Conv2D(\n",
    "                filters = 32, # number of filters\n",
    "                kernel_size = (3,3), # height/width of filter\n",
    "                activation = 'relu' # activation function \n",
    "                ))\n",
    "model.add(Conv2D(\n",
    "                filters = 32, # number of filters\n",
    "                kernel_size = (3,3), # height/width of filter\n",
    "                activation = 'relu' # activation function \n",
    "                ))\n",
    "\n",
    "# Add last pooling layer.\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Adding first dense layer with 256 nodes\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer to avoid overfitting\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# adding output layer\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=[tf.keras.metrics.AUC(curve = 'PR')])\n",
    "\n",
    "# fitting the model\n",
    "model.fit(X_train,\n",
    "            y_train,\n",
    "            batch_size=800,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=24)\n",
    "\n",
    "# evaluate the model \n",
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d18156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('yourmodelname.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc55ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e42016",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_model = keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbd3c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "if not cap.isOpened():\n",
    " raise IOError('Cannot open webcam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dcdba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 275 for command:\n",
      "        open rooster.mov\n",
      "    Cannot find the specified file.  Make sure the path and filename are correct.\n",
      "\n",
      "    Error 263 for command:\n",
      "        close rooster.mov\n",
      "    The specified device is not open or is not recognized by MCI.\n",
      "Failed to close the file: rooster.mov\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 275 for command:\n        open rooster.mov\n    Cannot find the specified file.  Make sure the path and filename are correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 74>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    128\u001b[0m k \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m## Sound\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrooster.mov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\playsound.py:72\u001b[0m, in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mwinCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     winCommand(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplay \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sound, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m wait\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     74\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\playsound.py:64\u001b[0m, in \u001b[0;36m_playsoundWin.<locals>.winCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     60\u001b[0m     exceptionMessage \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    Error \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(errorCode) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for command:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m command\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     62\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m errorBuffer\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     63\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(exceptionMessage)\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(exceptionMessage)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 275 for command:\n        open rooster.mov\n    Cannot find the specified file.  Make sure the path and filename are correct."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from playsound import playsound\n",
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "from tensorflow import keras\n",
    "eye_model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# webcam frame is inputted into function\n",
    "def eye_cropper(frame):\n",
    "\n",
    "    # create a variable for the facial feature coordinates\n",
    "    facial_features_list = face_recognition.face_landmarks(frame)\n",
    "\n",
    "    # create a placeholder list for the eye coordinates\n",
    "    # and append coordinates for eyes to list unless eyes\n",
    "    # weren't found by facial recognition\n",
    "    try:\n",
    "        eye = facial_features_list[0]['left_eye']\n",
    "    except:\n",
    "        try:\n",
    "            eye = facial_features_list[0]['right_eye']\n",
    "        except:\n",
    "            return\n",
    "    \n",
    "    # establish the max x and y coordinates of the eye\n",
    "    x_max = max([coordinate[0] for coordinate in eye])\n",
    "    x_min = min([coordinate[0] for coordinate in eye])\n",
    "    y_max = max([coordinate[1] for coordinate in eye])\n",
    "    y_min = min([coordinate[1] for coordinate in eye])\n",
    "\n",
    "    # establish the range of x and y coordinates\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "\n",
    "    # in order to make sure the full eye is captured,\n",
    "    # calculate the coordinates of a square that has a\n",
    "    # 50% cushion added to the axis with a larger range and\n",
    "    # then match the smaller range to the cushioned larger range\n",
    "    if x_range > y_range:\n",
    "        right = round(.5*x_range) + x_max\n",
    "        left = x_min - round(.5*x_range)\n",
    "        bottom = round((((right-left) - y_range))/2) + y_max\n",
    "        top = y_min - round((((right-left) - y_range))/2)\n",
    "    else:\n",
    "        bottom = round(.5*y_range) + y_max\n",
    "        top = y_min - round(.5*y_range)\n",
    "        right = round((((bottom-top) - x_range))/2) + x_max\n",
    "        left = x_min - round((((bottom-top) - x_range))/2)\n",
    "\n",
    "    # crop the image according to the coordinates determined above\n",
    "    cropped = frame[top:(bottom + 1), left:(right + 1)]\n",
    "\n",
    "    # resize the image\n",
    "    cropped = cv2.resize(cropped, (80,80))\n",
    "    image_for_prediction = cropped.reshape(-1, 80, 80, 3)\n",
    "\n",
    "    return image_for_prediction\n",
    "\n",
    "# initiate webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "if not cap.isOpened():\n",
    "    raise IOError('Cannot open webcam')\n",
    "\n",
    "# set a counter\n",
    "counter = 0\n",
    "\n",
    "# create a while loop that runs while webcam is in use\n",
    "while True:\n",
    "\n",
    "    # capture frames being outputted by webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # use only every other frame to manage speed and memory usage\n",
    "    frame_count = 0\n",
    "    if frame_count == 0:\n",
    "        frame_count += 1\n",
    "        pass\n",
    "    else:\n",
    "        count = 0\n",
    "        continue\n",
    "\n",
    "    # function called on the frame\n",
    "    image_for_prediction = eye_cropper(frame)\n",
    "    try:\n",
    "        image_for_prediction = image_for_prediction/255.0\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # get prediction from model\n",
    "    prediction = eye_model.predict(image_for_prediction)\n",
    "\n",
    "    # Based on prediction, display either \"Open Eyes\" or \"Closed Eyes\"\n",
    "    if prediction < 0.5:\n",
    "        counter = 0\n",
    "        status = 'Open'\n",
    "\n",
    "        cv2.rectangle(frame, (round(w/2) - 110,20), (round(w/2) + 110, 80), (38,38,38), -1)\n",
    "\n",
    "        cv2.putText(frame, status, (round(w/2)-80,70), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255,0), 2, cv2.LINE_4)\n",
    "        x1, y1,w1,h1 = 0,0,175,75\n",
    "        ## Draw black backgroun rectangle\n",
    "        cv2.rectangle(frame, (x1,x1), (x1+w1-20, y1+h1-20), (0,0,0), -1)\n",
    "        ## Add text\n",
    "        cv2.putText(frame, 'Active', (x1 +int(w1/10), y1+int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255,0),2)\n",
    "    else:\n",
    "        counter = counter + 1\n",
    "        status = 'Closed'\n",
    "\n",
    "        cv2.rectangle(frame, (round(w/2) - 110,20), (round(w/2) + 110, 80), (38,38,38), -1)\n",
    "\n",
    "        cv2.putText(frame, status, (round(w/2)-104,70), cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 2, cv2.LINE_4)\n",
    "        x1, y1,w1,h1 = 0,0,175,75\n",
    "        ## Draw black backgroun rectangle\n",
    "        cv2.rectangle(frame, (x1,x1), (x1+w1-20, y1+h1-20), (0,0,0), -1)\n",
    "        ## Add text\n",
    "        cv2.putText(frame, 'Active', (x1 +int(w1/10), y1+int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255,0),2)\n",
    "\n",
    "        # if the counter is greater than 3, play and show alert that user is asleep\n",
    "        if counter > 2:\n",
    "\n",
    "            ## Draw black background rectangle\n",
    "            cv2.rectangle(frame, (round(w/2) - 160, round(h) - 200), (round(w/2) + 160, round(h) - 120), (0,0,255), -1)\n",
    "            cv2.putText(frame, 'DRIVER SLEEPING', (round(w/2)-136,round(h) - 146), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2, cv2.LINE_4)\n",
    "            cv2.imshow('Drowsiness Detection', frame)\n",
    "            k = cv2.waitKey(1)\n",
    "            ## Sound\n",
    "            playsound('rooster.mov')\n",
    "            counter = 1\n",
    "            continue\n",
    "    \n",
    "    cv2.imshow('Drowsiness Detection', frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d2214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
